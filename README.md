[![Generic badge](https://img.shields.io/badge/Made_With-Python-<COLOR>.svg)](https://shields.io/)
[![Generic badge](https://img.shields.io/badge/Library-pymc-red.svg)](https://shields.io/)
[![Linux](https://svgshare.com/i/Zhy.svg)](https://svgshare.com/i/Zhy.svg)
![visitor badge](https://visitor-badge.glitch.me/badge?page_id=o-ikne.BML-Project)

# **Bayesian Machine Learning Project**

## **1. Overview**
This [project](https://github.com/rbardenet/bml-course/blob/m2-lille/projects/papers.pdf) is part of the lecture on *Bayesian Machine Learning* tought by [Rémi BARDENET](https://rbardenet.github.io/). The idea is to pick a paper from a list of given papers, and read it with a critical mind. For instance, we will:
- (1) explain the contents of the paper
- (2) emphasize the strong and weak points of the paper
- (3) apply it to real data of our choice when applicable.

This project is done by Zakaria Boulkhir \& me. For more insights check out our report.

## **2. Article**
M. Welling and Y. W. Teh. Bayesian learning via stochastic gradient Langevin
dynamics. In Proceedings of the 28th international conference on machine learning
(ICML-11), pages 681–688, 2011. 
([link](http://people.ee.duke.edu/~lcarin/398_icmlpaper.pdf))
```
@inproceedings{welling2011bayesian,
  title={Bayesian learning via stochastic gradient Langevin dynamics},
  author={Welling, Max and Teh, Yee W},
  booktitle={Proceedings of the 28th international conference on machine learning (ICML-11)},
  pages={681--688},
  year={2011},
  organization={Citeseer}
}
```

## **3. Data**
The data for this project is

## **4. Experiments**

### **Mixture of Gaussians**

### **Logistic Regression**

### **Independent Components Analysis**

## **5. Installation**

To try our implementation in your own machine you need to install the following requirements:

```python
pip install -r requirements.txt
```
