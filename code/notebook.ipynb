{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03dda887-271f-47d2-b51a-40566bcbe54e",
   "metadata": {},
   "source": [
    "---\n",
    "### **<p style=\"text-align: center; text-decoration: underline;\">Bayesian Machine Learning Project</p>**\n",
    "# **<p style=\"text-align: center;\">Bayesian Learning via Stochastic Gradient Langevin Dynamics</p>**\n",
    "---\n",
    "\n",
    "> Realized by: *Zakaria Boulkhir* & *Omar Iken*.\n",
    "\n",
    "> Master 2, Data Science, Lille University.\n",
    "\n",
    "---\n",
    "\n",
    "### ■ __Overview__\n",
    "This [project](https://github.com/rbardenet/bml-course/blob/m2-lille/projects/papers.pdf) is part of the lecture on *Bayesian Machine Learning* tought by [Rémi BARDENET](https://rbardenet.github.io/). The idea is to pick a paper from a list of given papers, and read it with a critical mind. For instance, we will:\n",
    "- (1) explain the contents of the paper\n",
    "- (2) emphasize the strong and weak points of the paper\n",
    "- (3) apply it to real data of our choice when applicable.\n",
    "\n",
    "So this notebook concerns the implementation part of the project.\n",
    "\n",
    "### ■ __Article__\n",
    "M. Welling and Y. W. Teh. Bayesian learning via stochastic gradient Langevin\n",
    "dynamics. In Proceedings of the 28th international conference on machine learning\n",
    "(ICML-11), pages 681–688, 2011.\n",
    "\n",
    "The main article is available [here](http://people.ee.duke.edu/~lcarin/398_icmlpaper.pdf).\n",
    "\n",
    "### ■ **<a name=\"content\">Contents</a>**\n",
    "\n",
    "- [I. Dataset](#dataset)\n",
    "\n",
    "- [2. Implementation](#imp)\n",
    "\n",
    "    - [2.1. Preliminaries]()\n",
    "    \n",
    "    - [2.2. Stochastic Gradient Langevin Dynamics (SGLD)]()\n",
    "    \n",
    "    - [2.3. Posterior Sampling]()\n",
    "    \n",
    "    \n",
    "- [3. Evaluation](#eval)\n",
    "\n",
    "    - [3.1. Mixture of Gaussians]()\n",
    "    \n",
    "    - [3.2. Logistic Regression]()\n",
    "    \n",
    "    - [3.3. Independent Components Analysis]()\n",
    "\n",
    "### ■ **Libraries**\n",
    "Let's start by uploading all the libraries needed for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2025ee14-0e92-445a-a813-f2195257478f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## numpy to handle arrays & matices\n",
    "import numpy as np\n",
    "\n",
    "## matplotlib & Seaborn to plot figures\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## pandas to handle dataframes\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "#-----------< Setting >------------#\n",
    "## set plots text font size & style\n",
    "sns.set(font_scale=1.2, style='whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7c8ce4-7989-4872-af09-4a463cf94cac",
   "metadata": {},
   "source": [
    "### ■ **<a name=\"dataset\">1. Dataset</a>** [(&#8593;)](#content)\n",
    "In this first section, we will load the dataset that we will use during this project. Then we will do some preprocessing and exploration.\n",
    "\n",
    "#### **1.2. Artificial Data**\n",
    "During this project, we will be working with both artificial and real data. The artificial data is generated as data-cases iid in six channels. Three channels had high kurtosis distributions while three others where normally distributed. The total number of generated samples is 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b81b92e4-51a8-44ba-80b0-e26f69b842b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_art_data(n_samples):\n",
    "    \"\"\"generate artificial data\"\"\"\n",
    "    \n",
    "    normal_samples = np.random.normal(size=(n_samples, n_samples, 3))\n",
    "    \n",
    "    kurtosis_samples = stats.kurtosis(np.random.normal(size=(n_samples, n_samples, n_samples, 3)), fisher=True)\n",
    "\n",
    "    data = np.concatenate((normal_samples, kurtosis_samples), axis=-1)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80782207-69c8-48d4-8746-75b6fa98008c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100, 6)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = generate_art_data(100)\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ee5297-425d-416c-b888-06fc50b49c56",
   "metadata": {},
   "source": [
    "#### **1.2. MEG Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cb4908-791e-4d75-82bf-921813a56a54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2cf0f46a-3543-4707-bc5c-43941ad09e46",
   "metadata": {},
   "source": [
    "### ■ **<a name=\"imp\">2. Implementation</a>** [(&#8593;)](#content)\n",
    "This second section concers implementing SGDL. The main goal is to do bayesian learning from large scale datasets. The main idea is to combine\n",
    "two type of algorithms:\n",
    "- *Stochastic Gradient Descent* algorithm (aka. Robbins-Monro) [\\ref{SGD}]: which stochastically optimize a likelihood.\n",
    "- *Langevin dynamics* [\\ref{LD}]: which injects noise into the parameter updates in such a way that the trajectory of the parameters will converge to the full posterior distribution rather than just the maximum a posteriori mode.\n",
    "\n",
    "The resulting algorithm starts off being similar to stochastic optimization, then automatically transitions to one that simulates samples from the posterior using Langevin dynamics.\n",
    "\n",
    "$$\n",
    "\\tag{1}\n",
    "\\label{SGD}\n",
    "\\Delta\\theta_t = \\frac{\\epsilon_t}{2}\\left(\\nabla\\log p(\\theta_t) + \\frac{N}{n}\\sum_{i=1}^{n}\\nabla\\log p(x_{ti}|\\theta_t)\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\tag{2}\n",
    "\\label{LD}\n",
    "\\Delta\\theta_t = \\frac{\\epsilon}{2}\\left(\\nabla\\log p(\\theta_t) + \\sum_{i=1}^{N}\\nabla\\log p(x_{i}|\\theta_t)\\right) + \\eta_t, \\qquad \\eta_t\\sim\\mathcal{N}(0, \\epsilon)\n",
    "$$\n",
    "\n",
    "In order to this method to work, i.e. two ensure that the method will converge to a local maximum, two main are required, and they concern the step size:\n",
    "1. $\\sum_{t=1}^{\\infty}\\epsilon_t = \\infty$: to ensure that the parameters will reah the high probability reagions whatever the starting point.\n",
    "2. $\\sum_{t=1}^{\\infty}\\epsilon_t^2 < \\infty$: to ensure that the parameters will converge to the mode and not just bouncing around it.\n",
    "\n",
    "#### **2.1. Preliminaries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bd5213-9f3a-4c37-9e68-9739918b3bca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4be72e2-d70e-4ee1-9708-e4195fef3ebb",
   "metadata": {},
   "source": [
    "#### **2.2. Stochastic Gradient Langevin Dynamics (SGLD)**\n",
    "\n",
    "Based on the similarities between the two algorithms (1) and (3), the idea is to combine the two approaches. This will lead to an efficient use of large data sets while allowing to take into account the uncertainty of the parameters in a Bayesian way.\n",
    "\n",
    "The process is as follows: \n",
    "1. use stochastic gradients descent\n",
    "2. add an amount of Gaussian noise balanced against the parameter uncertainty.\n",
    "3. allow the step sizes to go to zero. :\n",
    "\n",
    "Thus the proposed update is given by:\n",
    "\n",
    "$$\n",
    "\\tag{3}\n",
    "\\label{SGDL}\n",
    "\\Delta\\theta_t = \\frac{\\epsilon_t}{2}\\left(\\nabla\\log p(\\theta_t) + \\frac{N}{n}\\sum_{i=1}^{n}\\nabla\\log p(x_{ti}|\\theta_t)\\right) + \\eta_t, \\qquad \\eta_t\\sim\\mathcal{N}(0, \\epsilon)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a267e41-f6a1-4749-9c0b-f5643f50da24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9add87f6-63b3-479a-9c23-28e51ea5656c",
   "metadata": {},
   "source": [
    "#### **2.3. Posterior Sampling**\n",
    "\n",
    "The idea is that the samples collection should start after the algorithm has entered its posterior sampling phase, which will not happen until after it becomes Langevin dynamics. The main condition to ensure that the algorithm is in its posterior sampling phase is given by:\n",
    "\n",
    "$$\n",
    "\\tag{4}\n",
    "\\label{cond}\n",
    "\\alpha = \\frac{\\epsilon_tN^2}{4n}\\lambda_{max}(M^{1/2}V_sM^{1/2}) \\ll 1\n",
    "$$\n",
    "Where $\\lambda_{max}$ is the larget eignevalue, $M$ is the preconditioning matrix and $V$ is emprical variance of the paramaters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5418317b-775a-48ac-837a-e8b31ed04c26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93ef2cba-db54-41ae-87e8-17946e44bd9e",
   "metadata": {},
   "source": [
    "### ■ **<a name=\"eval\">3. Evaluation</a>** [(&#8593;)](#content)\n",
    "In this last section, we evaluate our algorithm implemented above on some classical examples. We start by a mixture of Gaussians, then we appy it on a Bayesian logistic regression model and at last on the Independent Components Analysis model.\n",
    "\n",
    "#### **3.1. Mixture of Gaussians**\n",
    "To show that our method works well, we start by applying it on a very basic and simple example with two parameters. This first example is the mixture of Gaussians:\n",
    "- $\\theta_1\\sim N(0, \\sigma_1^2)$ and $\\theta_2\\sim N(0, \\sigma_2^2)$ where $\\sigma_1^2=10$, $\\sigma_2^2=1$ \n",
    "- $x_i\\sim N(\\theta_1, \\sigma_x^2) + \\dfrac12 N(\\theta_1+\\theta_2, \\sigma_x^2)$ where $\\sigma_x^2=2$\n",
    "\n",
    "We draw 100 data points from the model with $\\theta_1=0$ and $\\theta_2=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f12bad-ba92-4c03-becd-6ffb195e9902",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d781bca-ca04-456e-9fd4-8f2d004ed893",
   "metadata": {},
   "source": [
    "#### **3.2. Logistic Regression**\n",
    "For this second example, we apply stochastic gradient Langevin algorithm to a Bayesian logistic regression model.\n",
    "We will be using the data from the *UCI* dataset, more specificaly, the *a9a* dataset which consists of 32561 observations and 123 features.\n",
    "\n",
    "$$\n",
    "p(y_i|x_i) = \\sigma(y_i\\beta^Tx_i)\n",
    "$$\n",
    "Where $\\beta$ are the parameters, and $\\sigma$ is the sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eba3747-a227-4427-b279-e2d9db60baae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35d60121-9ace-4f69-a480-99303560a60b",
   "metadata": {},
   "source": [
    "#### **3.3. Independent Components Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c000666b-816f-406e-b0ef-e25e55b401a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "517e7e58-2687-4a82-afc7-fa0df02dd585",
   "metadata": {},
   "source": [
    "### **References** \n",
    "@inproceedings{welling2011bayesian,\n",
    "  title={Bayesian learning via stochastic gradient Langevin dynamics},\n",
    "  author={Welling, Max and Teh, Yee W},\n",
    "  booktitle={Proceedings of the 28th international conference on machine learning (ICML-11)},\n",
    "  pages={681--688},\n",
    "  year={2011},\n",
    "  organization={Citeseer}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19820c59-eb37-4bc6-8d3c-6ea8f265a4f2",
   "metadata": {},
   "source": [
    "---\n",
    "<p style=\"text-align: center;\">Copyright © 2021 Omar Ikne & Zakaria Boulkhir</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
